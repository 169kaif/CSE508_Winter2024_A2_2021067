{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7d901e-7506-4903-9c4a-edea8569bb29",
   "metadata": {},
   "source": [
    "<h1>text feature extraction</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88e0b6f7-aeba-427f-a3cd-8ef896014cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e16a3be-7304-4c43-ae47-c642b8716db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/mo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/mo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the nltk punkt corpus for tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "# download the stopword corpus to get rid of stopwords later\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# download wordnet lemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1fbb8b-4b3e-4d1f-b566-eb3dbd1ada94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'Image', 'Review Text']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to read csv files\n",
    "\n",
    "def read_csv(file_name):\n",
    "    \n",
    "    # check if the file exists\n",
    "    if os.path.exists(file_name):\n",
    "\n",
    "        # open the file\n",
    "        with open(file_name, 'r') as file:\n",
    "\n",
    "            # read the file\n",
    "            reader = csv.reader(file)\n",
    "\n",
    "            # return the data\n",
    "            return list(reader)\n",
    "        \n",
    "    return None\n",
    "\n",
    "# read the csv file\n",
    "num_image_review = read_csv('A2_Data.csv')\n",
    "\n",
    "# remove the first row (fieldnames)\n",
    "num_image_review.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc40aa4-0101-423c-ad0b-2cb84c71a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dict for (prod_id : review)\n",
    "\n",
    "prod_reviews = dict()\n",
    "corrupted_prod_ids = ['2912', '2235', '2088', '3474', '2265', '3317']\n",
    "\n",
    "for ele in num_image_review:\n",
    "    prod_id = ele[0]\n",
    "    prod_review = ele[2]\n",
    "\n",
    "    # check if product_id is corrupted\n",
    "    if prod_id in corrupted_prod_ids:\n",
    "        continue\n",
    "    else: # add to prod reviews dictionary\n",
    "        prod_reviews[int(prod_id)] = prod_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f3562a7-5ba6-4366-b6b8-7d0e768c6d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aloha from hawaii!  it's 2015 and after 58 years, my '57 Fender P-Bass has a new case! the Fender P-Bass fits like a charm!\n",
      "['aloha', 'hawaii', '2015', '58', 'year', '57', 'fender', 'pbass', 'new', 'case', 'fender', 'pbass', 'fit', 'like', 'charm']\n"
     ]
    }
   ],
   "source": [
    "# apply preprocessing techniques to the review text\n",
    "\n",
    "print(prod_reviews[100])\n",
    "\n",
    "#use regex to look for the given patterns and remove them \n",
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+') \n",
    "    return re.sub(pattern, r'', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return re.sub(pattern, r'', text)\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    # get rid of links\n",
    "    text = remove_url(text)\n",
    "\n",
    "    # get rid of html classes\n",
    "    text = remove_html(text)\n",
    "\n",
    "    # remove punctuation\n",
    "    text = \"\".join([i for i in text if i not in string.punctuation])\n",
    "\n",
    "    # make it lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # tokenize the text\n",
    "    text = word_tokenize(text)\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    text_wo_sw = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            text_wo_sw.append(word)\n",
    "\n",
    "    text = text_wo_sw\n",
    "\n",
    "    # apply lemmatization\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    # apply stemming\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    text = [porter_stemmer.stem(word) for word in text]\n",
    "\n",
    "    return text\n",
    "\n",
    "for id, review in prod_reviews.items():\n",
    "    prod_reviews[id] = preprocess(review)\n",
    "\n",
    "print(prod_reviews[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b465cc14-0b9f-4aee-9fc5-6779ed3eb3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5019\n",
      "994\n"
     ]
    }
   ],
   "source": [
    "# build term frequency matrix\n",
    "\n",
    "# find unique words throughout the entire corpus\n",
    "unique_words = set()\n",
    "\n",
    "for review in prod_reviews.values():\n",
    "    for word in review:\n",
    "        unique_words.add(word)\n",
    "\n",
    "num_unique_words = len(unique_words)\n",
    "num_docs = len(prod_reviews)\n",
    "unique_words = list(unique_words)\n",
    "\n",
    "print(num_unique_words)\n",
    "print(num_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c263d5-ebc4-4e51-b66b-ded934406804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
